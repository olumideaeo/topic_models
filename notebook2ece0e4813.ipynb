{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"conda create -n cluster_topic_model python=3.7 -y\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-02T04:56:10.190944Z","iopub.execute_input":"2022-05-02T04:56:10.191207Z","iopub.status.idle":"2022-05-02T04:56:23.160997Z","shell.execute_reply.started":"2022-05-02T04:56:10.191176Z","shell.execute_reply":"2022-05-02T04:56:23.160168Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"!source activate cluster_topic_model","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:23.163008Z","iopub.execute_input":"2022-05-02T04:56:23.163200Z","iopub.status.idle":"2022-05-02T04:56:24.040267Z","shell.execute_reply.started":"2022-05-02T04:56:23.163176Z","shell.execute_reply":"2022-05-02T04:56:24.039344Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/hyintell/topicx.git","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:24.043353Z","iopub.execute_input":"2022-05-02T04:56:24.043649Z","iopub.status.idle":"2022-05-02T04:56:24.709111Z","shell.execute_reply.started":"2022-05-02T04:56:24.043609Z","shell.execute_reply":"2022-05-02T04:56:24.708318Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"!pip install -r ./topicx/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:24.713406Z","iopub.execute_input":"2022-05-02T04:56:24.713632Z","iopub.status.idle":"2022-05-02T04:56:34.427297Z","shell.execute_reply.started":"2022-05-02T04:56:24.713604Z","shell.execute_reply":"2022-05-02T04:56:34.426470Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"class TopicModel:\n\n    def __init__(self, dataset, topic_model, num_topics):\n        self.dataset = dataset\n        self.topic_model = topic_model\n        self.num_topics = num_topics\n        \n        \n    def train(self):\n        raise NotImplementedError(\"Train function has not been defined!\")\n        \n        \n    def evaluate(self):\n        raise NotImplementedError(\"Evaluate function has not been defined!\")\n        \n    \n    def get_topics(self):\n        raise NotImplementedError(\"Get topics function has not been defined!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import normalize\nimport numpy as np\nimport scipy.sparse as sp\n\n\nclass TFi(TfidfTransformer):\n\n    def __init__(self, X_per_cluster, *args, **kwargs):\n        print('====== Using TFi ======')\n        super().__init__(*args, **kwargs)\n        self.X_per_cluster = X_per_cluster\n        \n    \n    def socre(self):\n        \n        self._tfi = normalize(self.X_per_cluster, axis=1, norm='l1', copy=False)\n        scores = sp.csr_matrix(self._tfi)\n        \n        return scores ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.440425Z","iopub.execute_input":"2022-05-02T04:56:34.440774Z","iopub.status.idle":"2022-05-02T04:56:34.450525Z","shell.execute_reply.started":"2022-05-02T04:56:34.440739Z","shell.execute_reply":"2022-05-02T04:56:34.449198Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import normalize\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\n\n\nclass TFIDF_IDFi(TfidfTransformer):\n\n    def __init__(self, X_per_cluster, X_origin, all_documents, *args, **kwargs):\n        print('====== Using TFIDF_IDFi ======')\n        super().__init__(*args, **kwargs)\n        self.X_per_cluster = X_per_cluster\n        self.X_origin = X_origin\n        self.all_documents = all_documents\n        \n    \n    def socre(self):\n        \n        self._global_tfidf = self.fit_transform(self.X_origin)\n        \n        global_df = pd.DataFrame(self._global_tfidf.toarray())\n        global_df['Topic'] = self.all_documents.Topic\n        \n        avg_global_df = global_df.groupby(['Topic'], as_index=False).mean()\n        avg_global_df = avg_global_df.drop('Topic', 1)\n        self._avg_global_tfidf = avg_global_df.values\n        \n        local_tfidf_transformer = TfidfTransformer()\n        local_tfidf_transformer.fit_transform(self.X_per_cluster)\n        self._idfi = local_tfidf_transformer.idf_\n        \n        scores = self._avg_global_tfidf * self._idfi\n        scores = normalize(scores, axis=1, norm='l1', copy=False)\n        scores = sp.csr_matrix(scores)\n\n        return scores ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.452357Z","iopub.execute_input":"2022-05-02T04:56:34.452610Z","iopub.status.idle":"2022-05-02T04:56:34.462313Z","shell.execute_reply.started":"2022-05-02T04:56:34.452578Z","shell.execute_reply":"2022-05-02T04:56:34.461446Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.preprocessing import normalize\nimport numpy as np\nimport pandas as pd\nimport scipy.sparse as sp\n\n\nclass TFIDF_TFi(TfidfTransformer):\n\n    def __init__(self, X_per_cluster, X_origin, all_documents, *args, **kwargs):\n        print('====== Using TFIDF_TFi ======')\n        super().__init__(*args, **kwargs)\n        self.X_per_cluster = X_per_cluster\n        self.X_origin = X_origin\n        self.all_documents = all_documents\n        \n    \n    def socre(self):\n        \n        self._global_tfidf = self.fit_transform(self.X_origin)\n        \n        global_df = pd.DataFrame(self._global_tfidf.toarray())\n        global_df['Topic'] = self.all_documents.Topic\n        \n        avg_global_df = global_df.groupby(['Topic'], as_index=False).mean()\n        avg_global_df = avg_global_df.drop('Topic', 1)\n        self._avg_global_tfidf = avg_global_df.values\n        \n        # k * vocab\n        self._tfi = normalize(self.X_per_cluster, axis=1, norm='l1', copy=False).toarray()\n\n        scores = self._avg_global_tfidf * self._tfi\n        scores = normalize(scores, axis=1, norm='l1', copy=False)\n        scores = sp.csr_matrix(scores)\n        \n        return scores \n","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.464248Z","iopub.execute_input":"2022-05-02T04:56:34.464974Z","iopub.status.idle":"2022-05-02T04:56:34.477327Z","shell.execute_reply.started":"2022-05-02T04:56:34.464946Z","shell.execute_reply":"2022-05-02T04:56:34.476656Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfTransformer\nimport numpy as np\nimport scipy.sparse as sp\n\n\nclass TFIDFi(TfidfTransformer):\n\n    def __init__(self, X_per_cluster, *args, **kwargs):\n        print('====== Using TFIDFi ======')\n        super().__init__(*args, **kwargs)\n        self.X_per_cluster = X_per_cluster\n        \n    \n    def socre(self):\n        \n        self._tfidfi = self.fit_transform(self.X_per_cluster)\n        scores = sp.csr_matrix(self._tfidfi)\n        \n        return scores ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.478464Z","iopub.execute_input":"2022-05-02T04:56:34.478781Z","iopub.status.idle":"2022-05-02T04:56:34.488143Z","shell.execute_reply.started":"2022-05-02T04:56:34.478747Z","shell.execute_reply":"2022-05-02T04:56:34.487320Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom typing import List\n\n\nclass BaseEmbedder:\n    \"\"\" The Base Embedder used for creating embedding models\n    Arguments:\n        embedding_model: The main embedding model to be used for extracting\n                         document and word embedding\n        word_embedding_model: The embedding model used for extracting word\n                              embeddings only. If this model is selected,\n                              then the `embedding_model` is purely used for\n                              creating document embeddings.\n    \"\"\"\n    def __init__(self,\n                 embedding_model=None,\n                 word_embedding_model=None):\n        self.embedding_model = embedding_model\n        self.word_embedding_model = word_embedding_model\n\n    def embed(self,\n              documents: List[str],\n              verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n documents/words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            documents: A list of documents or words to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Document/words embeddings with shape (n, m) with `n` documents/words\n            that each have an embeddings size of `m`\n        \"\"\"\n        pass\n\n    def embed_words(self,\n                    words: List[str],\n                    verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            words: A list of words to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Word embeddings with shape (n, m) with `n` words\n            that each have an embeddings size of `m`\n        \"\"\"\n        return self.embed(words, verbose)\n\n    def embed_documents(self,\n                        document: List[str],\n                        verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            document: A list of documents to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Document embeddings with shape (n, m) with `n` documents\n            that each have an embeddings size of `m`\n        \"\"\"\n        return self.embed(document, verbose)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.491143Z","iopub.execute_input":"2022-05-02T04:56:34.494562Z","iopub.status.idle":"2022-05-02T04:56:34.504192Z","shell.execute_reply.started":"2022-05-02T04:56:34.494536Z","shell.execute_reply":"2022-05-02T04:56:34.503507Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom typing import List\n\n\nclass BaseEmbedder:\n    \"\"\" The Base Embedder used for creating embedding models\n    Arguments:\n        embedding_model: The main embedding model to be used for extracting\n                         document and word embedding\n        word_embedding_model: The embedding model used for extracting word\n                              embeddings only. If this model is selected,\n                              then the `embedding_model` is purely used for\n                              creating document embeddings.\n    \"\"\"\n    def __init__(self,\n                 embedding_model=None,\n                 word_embedding_model=None):\n        self.embedding_model = embedding_model\n        self.word_embedding_model = word_embedding_model\n\n    def embed(self,\n              documents: List[str],\n              verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n documents/words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            documents: A list of documents or words to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Document/words embeddings with shape (n, m) with `n` documents/words\n            that each have an embeddings size of `m`\n        \"\"\"\n        pass\n\n    def embed_words(self,\n                    words: List[str],\n                    verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            words: A list of words to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Word embeddings with shape (n, m) with `n` words\n            that each have an embeddings size of `m`\n        \"\"\"\n        return self.embed(words, verbose)\n\n    def embed_documents(self,\n                        document: List[str],\n                        verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            document: A list of documents to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Document embeddings with shape (n, m) with `n` documents\n            that each have an embeddings size of `m`\n        \"\"\"\n        return self.embed(document, verbose)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.506421Z","iopub.execute_input":"2022-05-02T04:56:34.507079Z","iopub.status.idle":"2022-05-02T04:56:34.517213Z","shell.execute_reply.started":"2022-05-02T04:56:34.507036Z","shell.execute_reply":"2022-05-02T04:56:34.516399Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom typing import List, Union\nfrom sentence_transformers import SentenceTransformer\n\n#from ._base import BaseEmbedder\n\n\nclass SentenceTransformerBackend(BaseEmbedder):\n    \"\"\" Sentence-transformers embedding model\n    The sentence-transformers embedding model used for generating document and\n    word embeddings.\n    Arguments:\n        embedding_model: A sentence-transformers embedding model\n    Usage:\n    To create a model, you can load in a string pointing to a\n    sentence-transformers model:\n    ```python\n    from bertopic.backend import SentenceTransformerBackend\n    sentence_model = SentenceTransformerBackend(\"all-MiniLM-L6-v2\")\n    ```\n    or  you can instantiate a model yourself:\n    ```python\n    from bertopic.backend import SentenceTransformerBackend\n    from sentence_transformers import SentenceTransformer\n    embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    sentence_model = SentenceTransformerBackend(embedding_model)\n    ```\n    \"\"\"\n    def __init__(self, embedding_model: Union[str, SentenceTransformer]):\n        super().__init__()\n\n        if isinstance(embedding_model, SentenceTransformer):\n            self.embedding_model = embedding_model\n        elif isinstance(embedding_model, str):\n            self.embedding_model = SentenceTransformer(embedding_model)\n        else:\n            raise ValueError(\"Please select a correct SentenceTransformers model: \\n\"\n                             \"`from sentence_transformers import SentenceTransformer` \\n\"\n                             \"`model = SentenceTransformer('all-MiniLM-L6-v2')`\")\n\n    def embed(self,\n              documents: List[str],\n              verbose: bool = False) -> np.ndarray:\n        \"\"\" Embed a list of n documents/words into an n-dimensional\n        matrix of embeddings\n        Arguments:\n            documents: A list of documents or words to be embedded\n            verbose: Controls the verbosity of the process\n        Returns:\n            Document/words embeddings with shape (n, m) with `n` documents/words\n            that each have an embeddings size of `m`\n        \"\"\"\n        embeddings = self.embedding_model.encode(documents, show_progress_bar=verbose)\n        return ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.518634Z","iopub.execute_input":"2022-05-02T04:56:34.519175Z","iopub.status.idle":"2022-05-02T04:56:34.530206Z","shell.execute_reply.started":"2022-05-02T04:56:34.519137Z","shell.execute_reply":"2022-05-02T04:56:34.529527Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#from ._base import BaseEmbedder\n#from ._sentencetransformers import SentenceTransformerBackend\n\n\ndef select_backend(embedding_model):\n\n    # Flair word embeddings\n    if \"flair\" in str(type(embedding_model)):\n        from ._flair import FlairBackend\n        return FlairBackend(embedding_model)\n\n    # Create a Sentence Transformer model based on a string\n    if isinstance(embedding_model, str):\n        return SentenceTransformerBackend(embedding_model)","metadata":{"execution":{"iopub.status.busy":"2022-05-02T04:56:34.531600Z","iopub.execute_input":"2022-05-02T04:56:34.532120Z","iopub.status.idle":"2022-05-02T04:56:34.540928Z","shell.execute_reply.started":"2022-05-02T04:56:34.532085Z","shell.execute_reply":"2022-05-02T04:56:34.540186Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom scipy.sparse.csr import csr_matrix\n\nfrom umap import UMAP\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#from .tfi import TFi\n#from .tfidfi import TFIDFi\n#from .tfidf_idfi import TFIDF_IDFi\n#from .tfidf_tfi import TFIDF_TFi\n#from .backend._utils import select_backend\n\n\nclass CETopic:\n\n    def __init__(self, top_n_words=10, nr_topics=10, embedding_model=None, dim_size=-1, word_select_method=None, seed=42):\n        \n        self.topics = None\n        self.topic_sizes = None\n        self.top_n_words = top_n_words\n        self.nr_topics = nr_topics\n        self.word_select_method = word_select_method\n        self.embedding_model = embedding_model\n        self.vectorizer_model = CountVectorizer()\n        \n        self.dim_size = dim_size\n        self.umap = None\n        if self.dim_size != -1:\n            self.umap = UMAP(n_neighbors=15, n_components=self.dim_size, min_dist=0.0, metric='cosine', random_state=seed)\n        \n        # cluster\n        self.kmeans = KMeans(self.nr_topics, random_state=seed)\n\n        \n    def fit_transform(self, documents, embeddings=None):\n        \n        documents = pd.DataFrame({\"Document\": documents,\n                                  \"ID\": range(len(documents)),\n                                  \"Topic\": None})\n\n        if embeddings is None:\n            self.embedding_model = select_backend(self.embedding_model)\n            embeddings = self._extract_embeddings(documents.Document)\n        else:\n            if self.embedding_model is not None:\n                self.embedding_model = select_backend(self.embedding_model)\n\n        if self.umap is not None:\n            embeddings = self._reduce_dimensionality(embeddings)\n        \n        documents = self._cluster_embeddings(embeddings, documents)\n\n        self._extract_topics(documents)\n        predictions = documents.Topic.to_list()\n\n        return predictions\n\n\n    def get_topics(self):\n        return self.topics\n    \n\n    def get_topic(self, topic_id):\n        if topic_id in self.topics:\n            return self.topics[topic_id]\n        else:\n            return False\n\n\n    def _extract_embeddings(self, documents):\n        \n        embeddings = self.embedding_model.embed_documents(documents)\n\n        return embeddings\n    \n\n    def _reduce_dimensionality(self, embeddings):\n\n        self.umap.fit(embeddings)\n        reduced_embeddings = self.umap.transform(embeddings)\n        \n        return np.nan_to_num(reduced_embeddings)\n    \n\n    def _cluster_embeddings(self, embeddings, documents):\n        \n        self.kmeans.fit(embeddings)\n        documents['Topic'] = self.kmeans.labels_\n        self._update_topic_size(documents)\n\n        return documents\n\n\n    def _extract_topics(self, documents):\n        \n        documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n        self.scores, words = self._weighting_words(documents_per_topic, documents)\n        self.topics = self._extract_words_per_topic(words)\n\n\n    def _weighting_words(self, documents_per_topic, all_documents):\n        \n        concatenated_documents = self._preprocess_text(documents_per_topic.Document.values)\n        origin_documents = self._preprocess_text(all_documents.Document.values)\n        \n        # count the words in a cluster\n        self.vectorizer_model.fit(concatenated_documents)\n        words = self.vectorizer_model.get_feature_names()\n        \n        # k * vocab\n        X_per_cluster = self.vectorizer_model.transform(concatenated_documents)\n        # D * vocab\n        X_origin = self.vectorizer_model.transform(origin_documents)\n        \n        if self.word_select_method == 'tfidf_idfi':\n            socres = TFIDF_IDFi(X_per_cluster, X_origin, all_documents).socre()\n        elif self.word_select_method == 'tfidf_tfi':\n            socres = TFIDF_TFi(X_per_cluster, X_origin, all_documents).socre()\n        elif self.word_select_method == 'tfi':\n            socres = TFi(X_per_cluster).socre()\n        elif self.word_select_method == 'tfidfi':\n            socres = TFIDFi(X_per_cluster).socre()\n\n        return socres, words\n    \n\n    def _update_topic_size(self, documents):\n\n        sizes = documents.groupby(['Topic']).count().sort_values(\"Document\", ascending=False).reset_index()\n        self.topic_sizes = dict(zip(sizes.Topic, sizes.Document))\n        \n\n    def _extract_words_per_topic(self, words):\n\n        labels = sorted(list(self.topic_sizes.keys()))\n\n        indices = self._top_n_idx_sparse(self.scores, 30)\n        scores = self._top_n_values_sparse(self.scores, indices)\n        sorted_indices = np.argsort(scores, 1)\n        indices = np.take_along_axis(indices, sorted_indices, axis=1)\n        scores = np.take_along_axis(scores, sorted_indices, axis=1)\n\n        topics = {label: [(words[word_index], score)\n                          if word_index and score > 0\n                          else (\"\", 0.00001)\n                          for word_index, score in zip(indices[index][::-1], scores[index][::-1])\n                          ]\n                  for index, label in enumerate(labels)}\n\n        topics = {label: values[:self.top_n_words] for label, values in topics.items()}\n\n        return topics\n\n\n    def _preprocess_text(self, documents):\n        \"\"\" Basic preprocessing of text\n        Steps:\n            * Lower text\n            * Replace \\n and \\t with whitespace\n            * Only keep alpha-numerical characters\n        \"\"\"\n        cleaned_documents = [doc.lower() for doc in documents]\n        cleaned_documents = [doc.replace(\"\\n\", \" \") for doc in cleaned_documents]\n        cleaned_documents = [doc.replace(\"\\t\", \" \") for doc in cleaned_documents]\n\n        return cleaned_documents\n    \n\n    @staticmethod\n    def _top_n_idx_sparse(matrix, n):\n        \"\"\" Return indices of top n values in each row of a sparse matrix\n        Retrieved from:\n            https://stackoverflow.com/questions/49207275/finding-the-top-n-values-in-a-row-of-a-scipy-sparse-matrix\n        Args:\n            matrix: The sparse matrix from which to get the top n indices per row\n            n: The number of highest values to extract from each row\n        Returns:\n            indices: The top n indices per row\n        \"\"\"\n        indices = []\n        for le, ri in zip(matrix.indptr[:-1], matrix.indptr[1:]):\n            n_row_pick = min(n, ri - le)\n            values = matrix.indices[le + np.argpartition(matrix.data[le:ri], -n_row_pick)[-n_row_pick:]]\n            values = [values[index] if len(values) >= index + 1 else None for index in range(n)]\n            indices.append(values)\n        return np.array(indices)\n    \n\n    @staticmethod\n    def _top_n_values_sparse(matrix, indices):\n        \"\"\" Return the top n values for each row in a sparse matrix\n        Args:\n            matrix: The sparse matrix from which to get the top n indices per row\n            indices: The top n indices per row\n        Returns:\n            top_values: The top n scores per row\n        \"\"\"\n        top_values = []\n        for row, values in enumerate(indices):\n            scores = np.array([matrix[row, value] if value is not None else 0 for value in values])\n            top_values.append(scores)\n        return np.array(top_values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from baselines.topic_model import TopicModel\n#from baselines.cetopic import CETopic\nimport pandas as pd\nfrom simcse import SimCSE\nimport gensim.corpora as corpora\nfrom flair.embeddings import TransformerDocumentEmbeddings\nfrom gensim.models.coherencemodel import CoherenceModel\n\nclass CETopicTM(TopicModel):\n    def __init__(self, dataset, topic_model, num_topics, dim_size, word_select_method, embedding, seed):\n        super().__init__(dataset, topic_model, num_topics)\n        print(f'Initialize CETopicTM with num_topics={num_topics}, embedding={embedding}')\n        self.dim_size = dim_size\n        self.word_select_method = word_select_method\n        self.embedding = embedding\n        self.seed = seed\n        \n        # make sentences and token_lists\n        token_lists = self.dataset.get_corpus()\n        self.sentences = [' '.join(text_list) for text_list in token_lists]\n        \n        embedding_model = TransformerDocumentEmbeddings(embedding)\n        self.model = CETopic(embedding_model=embedding_model,\n                             nr_topics=num_topics, \n                             dim_size=self.dim_size, \n                             word_select_method=self.word_select_method, \n                             seed=self.seed)   \n  \n    def train(self):\n        self.topics = self.model.fit_transform(self.sentences)\n    \n    def evaluate(self):\n        td_score = self._calculate_topic_diversity()\n        cv_score, npmi_score = self._calculate_cv_npmi(self.sentences, self.topics)\n        \n        return td_score, cv_score, npmi_score\n    \n    def get_topics(self):\n        return self.model.get_topics()\n    \n    def _calculate_topic_diversity(self):\n        topic_keywords = self.model.get_topics()\n\n        bertopic_topics = []\n        for k,v in topic_keywords.items():\n            temp = []\n            for tup in v:\n                temp.append(tup[0])\n            bertopic_topics.append(temp)  \n\n        unique_words = set()\n        for topic in bertopic_topics:\n            unique_words = unique_words.union(set(topic[:10]))\n        td = len(unique_words) / (10 * len(bertopic_topics))\n\n        return td\n\n    def _calculate_cv_npmi(self, docs, topics): \n\n        doc = pd.DataFrame({\"Document\": docs,\n                        \"ID\": range(len(docs)),\n                        \"Topic\": topics})\n        documents_per_topic = doc.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n        cleaned_docs = self.model._preprocess_text(documents_per_topic.Document.values)\n\n        vectorizer = self.model.vectorizer_model\n        analyzer = vectorizer.build_analyzer()\n\n        words = vectorizer.get_feature_names()\n        tokens = [analyzer(doc) for doc in cleaned_docs]\n        dictionary = corpora.Dictionary(tokens)\n        corpus = [dictionary.doc2bow(token) for token in tokens]\n        topic_words = [[words for words, _ in self.model.get_topic(topic)] \n                    for topic in range(len(set(topics))-1)]\n\n        coherence_model = CoherenceModel(topics=topic_words, \n                                      texts=tokens, \n                                      corpus=corpus,\n                                      dictionary=dictionary, \n                                      coherence='c_v')\n        cv_coherence = coherence_model.get_coherence()\n\n        coherence_model_npmi = CoherenceModel(topics=topic_words, \n                                      texts=tokens, \n                                      corpus=corpus,\n                                      dictionary=dictionary, \n                                      coherence='c_npmi')\n        npmi_coherence = coherence_model_npmi.get_coherence()\n\n        return cv_coherence, npmi_coherence ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from os import environ, makedirs\nfrom os.path import exists, expanduser, join, splitext\nimport pickle\nimport sys\nimport codecs\nimport shutil\nimport requests\nimport json\n\n\"\"\"\nThis code is highly inspired by the scikit-learn strategy to download datasets\n\"\"\"\n\n\ndef get_data_home(data_home=None):\n    \"\"\"Return the path of the octis data dir.\n    By default the data dir is set to a folder named 'octis_data' in the\n    user home folder.\n    Alternatively, it can be set by the 'OCTIS_DATA' environment\n    variable or programmatically by giving an explicit folder path. The '~'\n    symbol is expanded to the user home folder.\n    If the folder does not already exist, it is automatically created.\n    Parameters\n    ----------\n    data_home : str | None\n        The path to octis data dir.\n    \"\"\"\n    if data_home is None:\n        data_home = environ.get('OCTIS_DATA', join('~', 'octis_data'))\n    data_home = expanduser(data_home)\n    if not exists(data_home):\n        makedirs(data_home)\n    return data_home\n\n\ndef _pkl_filepath(*args, **kwargs):\n    \"\"\"Ensure different filenames for Python 2 and Python 3 pickles\n    An object pickled under Python 3 cannot be loaded under Python 2. An object\n    pickled under Python 2 can sometimes not be loaded correctly under Python 3\n    because some Python 2 strings are decoded as Python 3 strings which can be\n    problematic for objects that use Python 2 strings as byte buffers for\n    numerical data instead of \"real\" strings.\n    Therefore, dataset loaders in octis use different files for pickles\n    manages by Python 2 and Python 3 in the same OCTIS_DATA folder so as\n    to avoid conflicts.\n    args[-1] is expected to be the \".pkl\" filename. Under Python 3, a suffix is\n    inserted before the extension to s\n    _pkl_filepath('/path/to/folder', 'filename.pkl') returns:\n      - /path/to/folder/filename.pkl under Python 2\n      - /path/to/folder/filename_py3.pkl under Python 3+\n    \"\"\"\n    py3_suffix = kwargs.get(\"py3_suffix\", \"_py3\")\n    basename, ext = splitext(args[-1])\n    if sys.version_info[0] >= 3:\n        basename += py3_suffix\n    new_args = args[:-1] + (basename + ext,)\n    return join(*new_args)\n\n\ndef download_dataset(dataset_name, target_dir, cache_path):\n    \"\"\"Download the 20 newsgroups data and stored it as a zipped pickle.\"\"\"\n    corpus_path = join(target_dir, \"corpus.tsv\")\n    metadata_path = join(target_dir, \"metadata.json\")\n    vocabulary_path = join(target_dir, \"vocabulary.txt\")\n\n    if not exists(target_dir):\n        makedirs(target_dir)\n\n    dataset_url = \"https://raw.githubusercontent.com/MIND-Lab/OCTIS/master/preprocessed_datasets/\" + dataset_name\n\n    corpus = requests.get(dataset_url + \"/corpus.tsv\")\n    metadata = requests.get(dataset_url + \"/metadata.json\")\n    vocabulary = requests.get(dataset_url + \"/vocabulary.txt\")\n\n    if corpus and metadata and vocabulary:\n        with open(corpus_path, 'w') as f:\n            f.write(corpus.text)\n        with open(metadata_path, 'w') as f:\n            f.write(metadata.text)\n        with open(vocabulary_path, 'w') as f:\n            f.write(vocabulary.text)\n\n        only_docs, labels, partition = [], [], []\n        for d in corpus.text.split(\"\\n\"):\n            if len(d.strip()) > 0:\n                dsplit = d.strip().split(\"\\t\")\n                only_docs.append(dsplit[0])\n                if len(dsplit) > 1:\n                    partition.append(dsplit[1])\n                    if len(dsplit) > 2:\n                        labels.append(dsplit[2])\n\n        vocab = [word for word in vocabulary.text.split(\"\\n\") if len(word) > 0]\n        metadata = json.loads(metadata.text)\n\n        metadata[\"info\"][\"name\"] = dataset_name\n\n        # Store a zipped pickle\n        cache = dict(corpus=only_docs, labels=labels, partitions=partition, metadata=metadata,\n                     vocabulary=vocab)\n        compressed_content = codecs.encode(pickle.dumps(cache), 'zlib_codec')\n        with open(cache_path, 'wb') as f:\n            f.write(compressed_content)\n\n        shutil.rmtree(target_dir)\n        return cache\n    else:\n        raise Exception(dataset_name + ' dataset not found')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:00:33.926434Z","iopub.execute_input":"2022-05-02T05:00:33.926885Z","iopub.status.idle":"2022-05-02T05:00:33.945393Z","shell.execute_reply.started":"2022-05-02T05:00:33.926847Z","shell.execute_reply":"2022-05-02T05:00:33.944679Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"import codecs\nimport json\nimport pickle\nfrom os.path import join, exists\nfrom pathlib import Path\n\nimport pandas as pd\n\n#from octis.dataset.downloader import get_data_home, _pkl_filepath, download_dataset\n\n\nclass Dataset:\n    \"\"\"\n    Dataset handles a dataset and offers methods to access, save and edit the dataset data\n    \"\"\"\n\n    def __init__(self, corpus=None, vocabulary=None, labels=None, metadata=None, document_indexes=None):\n        \"\"\"\n        Initialize a dataset, parameters are optional\n        if you want to load a dataset, initialize this\n        class with default values and use the load method\n        Parameters\n        ----------\n        corpus : corpus of the dataset\n        vocabulary : vocabulary of the dataset\n        labels : labels of the dataset\n        metadata : metadata of the dataset\n        \"\"\"\n        self.__corpus = corpus\n        self.__vocabulary = vocabulary\n        self.__metadata = metadata\n        self.__labels = labels\n        self.__original_indexes = document_indexes\n        self.dataset_path = None\n        self.is_cached = False\n\n    def get_corpus(self):\n        return self.__corpus\n\n    # Partitioned Corpus getter\n    def get_partitioned_corpus(self, use_validation=True):\n        if \"last-training-doc\" in self.__metadata:\n            last_training_doc = self.__metadata[\"last-training-doc\"]\n            if use_validation:\n                last_validation_doc = self.__metadata[\"last-validation-doc\"]\n                if self.__corpus is not None and last_training_doc != 0:\n                    train_corpus = []\n                    test_corpus = []\n                    validation_corpus = []\n\n                    for i in range(last_training_doc):\n                        train_corpus.append(self.__corpus[i])\n                    for i in range(last_training_doc, last_validation_doc):\n                        validation_corpus.append(self.__corpus[i])\n                    for i in range(last_validation_doc, len(self.__corpus)):\n                        test_corpus.append(self.__corpus[i])\n                    return train_corpus, validation_corpus, test_corpus\n            else:\n                if self.__corpus is not None and last_training_doc != 0:\n                    if \"last-validation-doc\" in self.__metadata.keys():\n                        last_validation_doc = self.__metadata[\"last-validation-doc\"]\n                    else:\n                        last_validation_doc = 0\n\n                    train_corpus = []\n                    test_corpus = []\n                    for i in range(last_training_doc):\n                        train_corpus.append(self.__corpus[i])\n\n                    if last_validation_doc != 0:\n                        for i in range(last_validation_doc, len(self.__corpus)):\n                            test_corpus.append(self.__corpus[i])\n                    else:\n                        for i in range(last_training_doc, len(self.__corpus)):\n                            test_corpus.append(self.__corpus[i])\n                    return train_corpus, test_corpus\n        else:\n            return [self.__corpus]\n\n\n    # Edges getter\n    def get_edges(self):\n        return self.__edges\n\n    # Labels getter\n    def get_labels(self):\n        return self.__labels\n\n    # Metadata getter\n    def get_metadata(self):\n        return self.__metadata\n\n    # Info getter\n    def get_info(self):\n        if \"info\" in self.__metadata:\n            return self.__metadata[\"info\"]\n        else:\n            return None\n\n    # Vocabulary getter\n    def get_vocabulary(self):\n        return self.__vocabulary\n\n    def _save_metadata(self, file_name):\n        \"\"\"\n        Saves metadata in json serialized format\n        Parameters\n        ----------\n        file_name : name of the file to write\n        Returns\n        -------\n        True if the data is saved\n        \"\"\"\n        data = self.get_metadata()\n        if data is not None:\n            with open(file_name, 'w') as outfile:\n                json.dump(data, outfile)\n                return True\n        else:\n            raise Exception(\"error in saving metadata\")\n\n    def _load_metadata(self, file_name):\n        \"\"\"\n        Loads metadata from json serialized format\n        Parameters\n        ----------\n        file_name : name of the file to read\n        \"\"\"\n        file = Path(file_name)\n        if file.is_file():\n            with open(file_name, 'r') as metadata_file:\n                metadata = json.load(metadata_file)\n            self.__metadata = metadata\n\n    def _load_corpus(self, file_name):\n        \"\"\"\n        Loads corpus from a file\n        Parameters\n        ----------\n        file_name : name of the file to read\n        \"\"\"\n        file = Path(file_name)\n        if file.is_file():\n            with open(file_name, 'r') as corpus_file:\n                corpus = [line.strip().split() for line in corpus_file]\n            self.__corpus = corpus\n        else:\n            raise Exception(\"error in loading corpus\")\n\n    def _save_edges(self, file_name):\n        \"\"\"\n        Saves edges in a file, a line for each document\n        Parameters\n        ----------\n        file_name : name of the file to write\n        \"\"\"\n        data = self.get_edges()\n        if data is not None:\n            with open(file_name, 'w') as outfile:\n                for element in data:\n                    outfile.write(\"%s\\n\" % element)\n        else:\n            raise Exception(\"error in saving edges\")\n\n    def _load_edges(self, file_name):\n        \"\"\"\n        Loads edges from a file\n        Parameters\n        ----------\n        file_name : name of the file to read\n        \"\"\"\n        file = Path(file_name)\n        if file.is_file():\n            with open(file_name, 'r') as edges_file:\n                edges = [line[0:len(line) - 1] for line in edges_file]\n            self.__edges = edges\n\n    def _save_labels(self, file_name):\n        \"\"\"\n        Saves the labels in a file, each line contains\n        the labels of a single document\n        Parameters\n        ----------\n        file_name : name of the file to write\n        \"\"\"\n        data = self.get_labels()\n        if data is not None:\n            with open(file_name, 'w') as outfile:\n                for element in data:\n                    outfile.write(\"%s\\n\" % json.dumps(element))\n        else:\n            raise Exception(\"error in saving labels\")\n\n    def _load_labels(self, file_name):\n        \"\"\"\n        Loads labels from a file\n        Parameters\n        ----------\n        file_name : name of the file to read\n        ----------\n        \"\"\"\n        file = Path(file_name)\n        if file.is_file():\n            with open(file_name, 'r') as labels_file:\n                labels = [json.loads(line.strip()) for line in labels_file]\n            self.__labels = labels\n\n    def _save_vocabulary(self, file_name):\n        \"\"\"\n        Saves vocabulary dictionary in a file\n        Parameters\n        ----------\n        file_name : name of the file to write\n        -------\n        \"\"\"\n        data = self.get_vocabulary()\n        if data is not None:\n            with open(file_name, 'w', encoding='utf8') as outfile:\n                for word in data:\n                    outfile.write(word + \"\\n\")\n        else:\n            raise Exception(\"error in saving vocabulary\")\n\n    def _save_document_indexes(self, file_name):\n        \"\"\"\n        Saves document indexes in a file\n        Parameters\n        ----------\n        file_name : name of the file to write\n        -------\n        \"\"\"\n        if self.__original_indexes is not None:\n            with open(file_name, 'w') as outfile:\n                for i in self.__original_indexes:\n                    outfile.write(str(i) + \"\\n\")\n\n    def _load_vocabulary(self, file_name):\n        \"\"\"\n        Loads vocabulary from a file\n        Parameters\n        ----------\n        file_name : name of the file to read\n        \"\"\"\n        vocabulary = []\n        file = Path(file_name)\n        if file.is_file():\n            with open(file_name, 'r') as vocabulary_file:\n                for line in vocabulary_file:\n                    vocabulary.append(line.strip())\n            self.__vocabulary = vocabulary\n        else:\n            raise Exception(\"error in loading vocabulary\")\n\n    def _load_document_indexes(self, file_name):\n        \"\"\"\n        Loads document indexes from a file\n        Parameters\n        ----------\n        file_name : name of the file to read\n        \"\"\"\n        document_indexes = []\n        file = Path(file_name)\n        if file.is_file():\n            with open(file_name, 'r') as indexes_file:\n                for line in indexes_file:\n                    document_indexes.append(line.strip())\n            self.__original_indexes = document_indexes\n        else:\n            raise Exception(\"error in loading vocabulary\")\n\n    def save(self, path, multilabel=False):\n        \"\"\"\n        Saves all the dataset info in a folder\n        Parameters\n        ----------\n        path : path to the folder in which files are saved.\n               If the folder doesn't exist it will be created\n        \"\"\"\n        Path(path).mkdir(parents=True, exist_ok=True)\n        try:\n            partitions = self.get_partitioned_corpus()\n            corpus, partition = [], []\n            for i, p in enumerate(partitions):\n                if i == 0:\n                    part = 'train'\n                elif i == 1 and len(partitions) == 3:\n                    part = 'val'\n                else:\n                    part = 'test'\n\n                for doc in p:\n                    corpus.append(' '.join(doc))\n                    partition.append(part)\n\n            df = pd.DataFrame(data=corpus)\n            df = pd.concat([df, pd.DataFrame(partition)], axis=1)\n\n            if multilabel:\n                labs = [' '.join(lab) for lab in self.__labels]\n            else:\n                labs = self.__labels\n            if self.__labels:\n                df = pd.concat([df, pd.DataFrame(labs)], axis=1)\n            df.to_csv(path + '/corpus.tsv', sep='\\t', index=False, header=False)\n\n            self._save_vocabulary(path + \"/vocabulary.txt\")\n            self._save_metadata(path + \"/metadata.json\")\n            self._save_document_indexes(path + \"/indexes.txt\")\n            self.dataset_path = path\n\n        except:\n            raise Exception(\"error in saving the dataset\")\n\n    def load_custom_dataset_from_folder(self, path, multilabel=False):\n        \"\"\"\n        Loads all the dataset from a folder\n        Parameters\n        ----------\n        path : path of the folder to read\n        \"\"\"\n        self.dataset_path = path\n        try:\n            if exists(self.dataset_path + \"/metadata.json\"):\n                self._load_metadata(self.dataset_path + \"/metadata.json\")\n            else:\n                self.__metadata = dict()\n            df = pd.read_csv(self.dataset_path + \"/corpus.tsv\", sep='\\t', header=None)\n            if len(df.keys()) > 1:\n                #just make sure docs are sorted in the right way (train - val - test)\n                final_df = df[df[1] == 'train'].append(df[df[1] == 'val'])\n                final_df = final_df.append(df[df[1] == 'test'])\n                self.__metadata['last-training-doc'] = len(final_df[final_df[1] == 'train'])\n                self.__metadata['last-validation-doc'] = len(final_df[final_df[1] == 'val']) + \\\n                                                         len(final_df[final_df[1] == 'train'])\n\n                self.__corpus = [d.split() for d in final_df[0].tolist()]\n                if len(final_df.keys()) > 2:\n                    if multilabel:\n                        self.__labels = [doc.split() for doc in final_df[2].tolist()]\n                    else:\n                        self.__labels = final_df[2].tolist()\n\n            else:\n                self.__corpus = [d.split() for d in df[0].tolist()]\n                self.__metadata['last-training-doc'] = len(df[0])\n\n            if exists(self.dataset_path + \"/vocabulary.txt\"):\n                self._load_vocabulary(self.dataset_path + \"/vocabulary.txt\")\n            else:\n                vocab = set()\n                for d in self.__corpus:\n                    for w in set(d):\n                        vocab.add(w)\n                self.__vocabulary = list(vocab)\n            if exists(self.dataset_path + \"/indexes.txt\"):\n                self._load_document_indexes(self.dataset_path + \"/indexes.txt\")\n        except:\n            raise Exception(\"error in loading the dataset:\" + self.dataset_path)\n\n    def fetch_dataset(self, dataset_name, data_home=None, download_if_missing=True):\n        \"\"\"Load the filenames and data from a dataset.\n        Parameters\n        ----------\n        dataset_name: name of the dataset to download or retrieve\n        data_home : optional, default: None\n            Specify a download and cache folder for the datasets. If None,\n            all data is stored in '~/octis' subfolders.\n        download_if_missing : optional, True by default\n            If False, raise an IOError if the data is not locally available\n            instead of trying to download the data from the source site.\n        \"\"\"\n\n        data_home = get_data_home(data_home=data_home)\n        cache_path = _pkl_filepath(data_home, dataset_name + \".pkz\")\n        dataset_home = join(data_home, dataset_name)\n        cache = None\n        if exists(cache_path):\n            try:\n                with open(cache_path, 'rb') as f:\n                    compressed_content = f.read()\n                uncompressed_content = codecs.decode(\n                    compressed_content, 'zlib_codec')\n                cache = pickle.loads(uncompressed_content)\n            except Exception as e:\n                print(80 * '_')\n                print('Cache loading failed')\n                print(80 * '_')\n                print(e)\n\n        if cache is None:\n            if download_if_missing:\n                cache = download_dataset(dataset_name, target_dir=dataset_home, cache_path=cache_path)\n            else:\n                raise IOError(dataset_name + ' dataset not found')\n        self.is_cached = True\n        self.__corpus = [d.split() for d in cache[\"corpus\"]]\n        self.__vocabulary = cache[\"vocabulary\"]\n        self.__metadata = cache[\"metadata\"]\n        self.dataset_path = cache_path\n        self.__labels = cache[\"labels\"]","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:00:42.209367Z","iopub.execute_input":"2022-05-02T05:00:42.209647Z","iopub.status.idle":"2022-05-02T05:00:42.267319Z","shell.execute_reply.started":"2022-05-02T05:00:42.209615Z","shell.execute_reply":"2022-05-02T05:00:42.266455Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#from octis.dataset.dataset import Dataset\n\ndef prepare_dataset(dataset_name):\n    \n    dataset = Dataset()\n    \n    if dataset_name == '20ng':\n        dataset.fetch_dataset('20NewsGroup')\n    elif dataset_name == 'bbc':\n        dataset.fetch_dataset('BBC_news')\n    elif dataset_name == 'm10':\n        dataset.fetch_dataset('M10')\n        \n    # make sentences and token_lists\n    token_lists = dataset.get_corpus()\n    sentences = [' '.join(text_list) for text_list in token_lists]\n    \n    return dataset, ","metadata":{"execution":{"iopub.status.busy":"2022-05-02T05:04:11.047205Z","iopub.execute_input":"2022-05-02T05:04:11.047483Z","iopub.status.idle":"2022-05-02T05:04:11.052821Z","shell.execute_reply.started":"2022-05-02T05:04:11.047440Z","shell.execute_reply":"2022-05-02T05:04:11.052113Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"#from baselines.cetopictm import CETopicTM\n#from utils import prepare_dataset\n\ndataset = prepare_dataset('bbc')\n\ntm = CETopicTM(dataset=dataset, \n               topic_model='cetopic', \n               num_topics=5, \n               dim_size=5, \n               word_select_method='tfidf_idfi',\n               embedding='princeton-nlp/unsup-simcse-bert-base-uncased', \n               seed=42)\n\ntm.train()\ntd_score, cv_score, npmi_score = tm.evaluate()\nprint(f'td: {td_score} npmi: {npmi_score} cv: {cv_score}')\n\ntopics = tm.get_topics()\nprint(f'Topics: {topics}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}